#!/usr/bin/env python3
"""
üé® ULTRA LUXURY PORTRAIT GENERATOR üé®
====================================
OPTIMIZED FOR RTX 4060 Ti 16GB - ULTRA FAST, ULTRA HIGH QUALITY

Features:
- ‚ö° 60-90 second generation times
- üß† AI prompt expansion via Ollama
- üéØ GGUF optimized workflows
- üì∏ 1024x1024 high quality output
- üîÑ Batch processing
- üõ°Ô∏è 100% bulletproof error handling

Usage:
  ./ultra_portrait_gen.py "Ukrainian wealth muse"
  ./ultra_portrait_gen.py --batch "financial goddess" "luxury entrepreneur" 
  ./ultra_portrait_gen.py --interactive
"""

import json
import time
import requests
import argparse
import sys
from pathlib import Path
from typing import List, Dict, Optional

# ============================================================================
# CONFIGURATION - OPTIMIZED FOR RTX 4060 Ti 16GB
# ============================================================================

CONFIG = {
    'comfyui_url': 'http://localhost:8188',
    'ollama_url': 'http://localhost:11434',
    'workflow_path': Path('workflows/flux_fast_gguf_portrait.json'),
    'output_dir': Path('ComfyUI/output'),
    'generation_timeout': 150,  # 2.5 minutes max
    'prompt_timeout': 15,       # 15 seconds for prompt generation
    'max_retries': 2,
    'poll_interval': 2,         # Check every 2 seconds
    'status_interval': 20       # Status update every 20 seconds
}

class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    END = '\033[0m'

# ============================================================================
# CORE FUNCTIONS
# ============================================================================

def log(message: str, level: str = 'info'):
    """Enhanced logging with colors and timestamps"""
    timestamp = time.strftime('%H:%M:%S')
    colors = {
        'info': Colors.CYAN,
        'success': Colors.GREEN,
        'warning': Colors.YELLOW,
        'error': Colors.RED,
        'highlight': Colors.BOLD + Colors.BLUE
    }
    color = colors.get(level, Colors.END)
    print(f"{Colors.BOLD}[{timestamp}]{Colors.END} {color}{message}{Colors.END}")

def check_services() -> bool:
    """Verify all required services are running"""
    log("üîç Checking services...", 'info')
    
    services = {
        'ComfyUI': f"{CONFIG['comfyui_url']}/system_stats",
        'Ollama': f"{CONFIG['ollama_url']}/api/tags"
    }
    
    for name, url in services.items():
        try:
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                log(f"‚úÖ {name} is running", 'success')
            else:
                log(f"‚ùå {name} error: HTTP {response.status_code}", 'error')
                return False
        except requests.RequestException as e:
            log(f"‚ùå {name} not accessible: {e}", 'error')
            return False
    
    return True

def generate_enhanced_prompt(concept: str, variation: int = 1) -> Optional[str]:
    """Generate enhanced prompt using Ollama with error handling"""
    log(f"üß† Generating prompt for: '{concept}' (variation {variation})", 'info')
    
    # Variation instructions for diversity
    variation_styles = {
        1: "Create a luxurious, modern setting with elegant lighting",
        2: "Focus on dramatic, cinematic mood with rich textures", 
        3: "Emphasize sophistication with minimalist, high-end aesthetics"
    }
    
    style_instruction = variation_styles.get(variation, variation_styles[1])
    full_prompt = f"{style_instruction} for: {concept}"
    
    payload = {
        "model": "prompter",
        "prompt": full_prompt,
        "stream": False,
        "options": {
            "temperature": 0.7 + (variation * 0.1),  # Slight variation
            "top_p": 0.9,
            "num_predict": 300  # Limit response length
        }
    }
    
    try:
        response = requests.post(
            f"{CONFIG['ollama_url']}/api/generate",
            json=payload,
            timeout=CONFIG['prompt_timeout']
        )
        
        if response.status_code == 200:
            result = response.json()
            enhanced_prompt = result['response'].strip()
            
            # Clean up the prompt
            if enhanced_prompt.startswith('"') and enhanced_prompt.endswith('"'):
                enhanced_prompt = enhanced_prompt[1:-1]
            
            log(f"‚úÖ Enhanced prompt generated ({len(enhanced_prompt)} chars)", 'success')
            return enhanced_prompt
        else:
            log(f"‚ùå Ollama error: HTTP {response.status_code}", 'error')
            return None
            
    except requests.RequestException as e:
        log(f"‚ùå Prompt generation failed: {e}", 'error')
        return None

def load_workflow_template() -> Optional[Dict]:
    """Load and validate workflow template"""
    try:
        with open(CONFIG['workflow_path'], 'r') as f:
            workflow = json.load(f)
        log(f"‚úÖ Workflow loaded: {CONFIG['workflow_path']}", 'success')
        return workflow
    except Exception as e:
        log(f"‚ùå Failed to load workflow: {e}", 'error')
        return None

def customize_workflow(template: Dict, prompt: str, filename: str, seed: int) -> Dict:
    """Customize workflow with specific parameters"""
    workflow = template.copy()
    
    # Update text prompt
    workflow['5']['inputs']['text'] = prompt
    
    # Update generation parameters optimized for RTX 4060 Ti
    workflow['8']['inputs'].update({
        'seed': seed,
        'steps': 10,        # Optimal for GGUF speed
        'cfg': 2.0,         # Faster convergence
        'sampler_name': 'euler',
        'scheduler': 'simple'
    })
    
    # Update filename
    workflow['10']['inputs']['filename_prefix'] = filename
    
    return workflow

def submit_generation(workflow: Dict) -> Optional[str]:
    """Submit generation to ComfyUI with retries"""
    payload = {
        'prompt': workflow,
        'client_id': f'ultra_gen_{int(time.time())}'
    }
    
    for attempt in range(CONFIG['max_retries']):
        try:
            response = requests.post(
                f"{CONFIG['comfyui_url']}/prompt",
                json=payload,
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                prompt_id = result['prompt_id']
                log(f"‚úÖ Generation submitted: {prompt_id}", 'success')
                return prompt_id
            else:
                log(f"‚ùå ComfyUI error: HTTP {response.status_code}", 'error')
                if attempt < CONFIG['max_retries'] - 1:
                    log(f"üîÑ Retrying... ({attempt + 1}/{CONFIG['max_retries']})", 'warning')
                    time.sleep(2)
                
        except requests.RequestException as e:
            log(f"‚ùå Submission error: {e}", 'error')
            if attempt < CONFIG['max_retries'] - 1:
                time.sleep(2)
    
    return None

def wait_for_completion(prompt_id: str, name: str) -> bool:
    """Wait for generation with enhanced progress tracking"""
    log(f"‚è≥ Waiting for '{name}' to complete...", 'info')
    start_time = time.time()
    last_status = 0
    
    while time.time() - start_time < CONFIG['generation_timeout']:
        try:
            response = requests.get(
                f"{CONFIG['comfyui_url']}/history/{prompt_id}",
                timeout=5
            )
            
            if response.status_code == 200:
                history = response.json()
                
                if prompt_id in history and history[prompt_id].get('outputs'):
                    elapsed = time.time() - start_time
                    
                    # Get generated files
                    outputs = history[prompt_id]['outputs']
                    files = []
                    for node_id, output in outputs.items():
                        if 'images' in output:
                            files.extend([img['filename'] for img in output['images']])
                    
                    log(f"üéâ '{name}' completed in {elapsed:.1f}s", 'success')
                    for file in files:
                        log(f"üì∏ Generated: {file}", 'highlight')
                    return True
            
            # Progress updates
            elapsed = time.time() - start_time
            if elapsed > CONFIG['status_interval'] and int(elapsed) % CONFIG['status_interval'] == 0 and elapsed != last_status:
                progress = min(90, (elapsed / CONFIG['generation_timeout']) * 100)
                log(f"‚ö° '{name}' generating... {progress:.0f}% ({elapsed:.0f}s)", 'info')
                last_status = elapsed
            
            time.sleep(CONFIG['poll_interval'])
            
        except requests.RequestException:
            time.sleep(CONFIG['poll_interval'])
    
    log(f"‚è∞ '{name}' timed out after {CONFIG['generation_timeout']}s", 'error')
    return False

def generate_portraits(concepts: List[str], variations: int = 1, batch_mode: bool = False) -> bool:
    """Main generation pipeline"""
    log(f"üöÄ ULTRA LUXURY PORTRAIT GENERATOR", 'highlight')
    log(f"üéØ Concepts: {len(concepts)} | Variations: {variations} | Batch: {batch_mode}", 'info')
    
    # Verify services
    if not check_services():
        log("‚ùå Services check failed", 'error')
        return False
    
    # Load workflow
    template = load_workflow_template()
    if not template:
        return False
    
    # Generate all prompts first
    generation_tasks = []
    
    for concept in concepts:
        for var in range(1, variations + 1):
            enhanced_prompt = generate_enhanced_prompt(concept, var)
            if enhanced_prompt:
                task = {
                    'concept': concept,
                    'variation': var,
                    'prompt': enhanced_prompt,
                    'filename': f"{concept.replace(' ', '_').lower()}_v{var}",
                    'seed': hash(concept + str(var)) % 100000
                }
                generation_tasks.append(task)
            else:
                log(f"‚ö†Ô∏è Skipping {concept} v{var} - prompt generation failed", 'warning')
    
    if not generation_tasks:
        log("‚ùå No valid generation tasks", 'error')
        return False
    
    log(f"üìã Generated {len(generation_tasks)} tasks", 'success')
    
    # Execute generations
    start_time = time.time()
    successful = 0
    
    if batch_mode:
        # Submit all, then wait
        pending_jobs = []
        
        for task in generation_tasks:
            workflow = customize_workflow(
                template, task['prompt'], task['filename'], task['seed']
            )
            prompt_id = submit_generation(workflow)
            if prompt_id:
                pending_jobs.append((prompt_id, task['filename']))
        
        log(f"üì§ Submitted {len(pending_jobs)} batch jobs", 'info')
        
        # Wait for all
        for prompt_id, filename in pending_jobs:
            if wait_for_completion(prompt_id, filename):
                successful += 1
    
    else:
        # Sequential processing
        for i, task in enumerate(generation_tasks, 1):
            log(f"\nüé® Processing {i}/{len(generation_tasks)}: {task['concept']} v{task['variation']}", 'highlight')
            
            workflow = customize_workflow(
                template, task['prompt'], task['filename'], task['seed']
            )
            
            prompt_id = submit_generation(workflow)
            if prompt_id and wait_for_completion(prompt_id, task['filename']):
                successful += 1
    
    # Final summary
    total_time = time.time() - start_time
    success_rate = (successful / len(generation_tasks)) * 100
    
    log(f"\nüèÅ GENERATION COMPLETE", 'highlight')
    log(f"‚úÖ Success: {successful}/{len(generation_tasks)} ({success_rate:.1f}%)", 'success')
    log(f"‚è±Ô∏è Total Time: {total_time:.1f}s", 'info')
    log(f"‚ö° Average: {total_time/len(generation_tasks):.1f}s per image", 'info')
    
    if successful > 0:
        log(f"üìÅ Images saved to: {CONFIG['output_dir']}/", 'success')
        return True
    
    return False

# ============================================================================
# CLI INTERFACE
# ============================================================================

def interactive_mode():
    """Interactive CLI mode with enhanced UX"""
    print(f"\n{Colors.HEADER}üé® ULTRA LUXURY PORTRAIT GENERATOR - INTERACTIVE MODE üé®{Colors.END}\n")
    
    log("Enter concepts one per line. Type 'done' when finished.", 'info')
    log("Examples: 'Ukrainian wealth muse', 'financial goddess', 'luxury entrepreneur'", 'highlight')
    
    concepts = []
    while True:
        try:
            concept = input(f"\n{Colors.BOLD}Concept {len(concepts)+1}:{Colors.END} ").strip()
            if concept.lower() == 'done':
                break
            elif concept:
                concepts.append(concept)
                log(f"Added: '{concept}'", 'success')
        except KeyboardInterrupt:
            log("\nüëã Goodbye!", 'info')
            return
    
    if not concepts:
        log("No concepts provided", 'warning')
        return
    
    # Get parameters
    try:
        variations = int(input(f"{Colors.BOLD}Variations per concept (1-3):{Colors.END} ") or "1")
        variations = max(1, min(3, variations))
        
        batch_input = input(f"{Colors.BOLD}Batch mode? (y/n):{Colors.END} ").lower()
        batch_mode = batch_input.startswith('y')
        
    except (ValueError, KeyboardInterrupt):
        log("Using default settings", 'info')
        variations = 1
        batch_mode = False
    
    # Run generation
    generate_portraits(concepts, variations, batch_mode)

def main():
    """Main CLI entry point"""
    parser = argparse.ArgumentParser(
        description='üé® Ultra Luxury Portrait Generator - RTX 4060 Ti Optimized',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  ./ultra_portrait_gen.py "Ukrainian wealth muse"
  ./ultra_portrait_gen.py --variations 2 "financial goddess" "luxury entrepreneur"
  ./ultra_portrait_gen.py --batch --variations 3 "exotic motivator" "business maven"
  ./ultra_portrait_gen.py --interactive
        """
    )
    
    parser.add_argument('concepts', nargs='*', help='Portrait concepts to generate')
    parser.add_argument('--variations', '-v', type=int, default=1, 
                       help='Variations per concept (1-3, default: 1)')
    parser.add_argument('--batch', '-b', action='store_true',
                       help='Batch mode (submit all, then wait)')
    parser.add_argument('--interactive', '-i', action='store_true',
                       help='Interactive mode')
    
    args = parser.parse_args()
    
    if args.interactive:
        interactive_mode()
    elif args.concepts:
        variations = max(1, min(3, args.variations))
        success = generate_portraits(args.concepts, variations, args.batch)
        sys.exit(0 if success else 1)
    else:
        log("‚ùå No concepts provided. Use --help for usage.", 'error')
        parser.print_help()
        sys.exit(1)

if __name__ == "__main__":
    main()